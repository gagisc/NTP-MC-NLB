# ARCHITECTURE.md - Detailed System Design

## System Architecture Overview

This document provides detailed technical architecture of the NTP-MC-NLB (NTP Multi-Cloud Load Balanced) infrastructure.

## 1. Network Topology

### 1.1 Cloud Regions

```
┌─────────────────────────────────────────────────────────────────┐
│                         INTERNET                                 │
│          NTP Pool Clients (global)                              │
│          pool.ntp.org Registration                              │
└────────────────────────┬────────────────────────────────────────┘
                         │
        ┌────────────────┼────────────────┐
        │                │                │
        ▼                ▼                ▼
   ┌─────────┐       ┌─────────┐   ┌──────────┐
   │ Route53 │       │  GCP    │   │   IPv6   │
   │(DNS)   │       │  LB     │   │  Anycast │
   └────┬────┘       └────┬────┘   └─────┬────┘
        │                 │              │
        │         ┌───────┴──────┬──────┘
        │         │              │
        ▼         ▼              ▼
    ┌──────────────────┐    ┌──────────────────┐
    │  AWS (us-east-1) │    │ GCP (us-central1)│
    ├──────────────────┤    ├──────────────────┤
    │ VPC: 10.0.0.0/16 │    │VPC: 10.1.0.0/16  │
    │                  │    │                  │
    │ EC2 Instances:   │    │Compute Instance: │
    │ • t2.micro CP    │    │ • e2-micro Agent │
    │ • t2.micro Ag    │    │                  │
    │                  │    │                  │
    │ Elastic IP:      │    │Ephemeral IP:     │
    │ • <static>       │    │ • <dynamic>      │
    │ • <static>       │    │                  │
    └────────┬─────────┘    └────────┬─────────┘
             │                       │
             └───────────┬───────────┘
                         │
                  WireGuard VPN
                (192.168.10.0/24)
                    Port 51820 UDP
                    AES-256 Encryption
```

### 1.2 WireGuard VPN Configuration

```
AWS Control Plane (192.168.10.1)
├── Private Key: (generated by Terraform)
├── Listen Port: 51820
└── Peers:
    ├── GCP Agent (192.168.10.2)
    ├── AWS Agent 1 (192.168.10.3)
    ├── AWS Agent 2 (192.168.10.4)
    └── ...

GCP Agent (192.168.10.2)
├── Private Key: (generated by Terraform)
├── Listen Port: 51820
└── Peer:
    └── AWS Control Plane (192.168.10.1)

AWS Agents (192.168.10.3+)
├── Private Keys: (generated by Terraform)
├── Listen Port: 51820
└── Peer:
    └── AWS Control Plane (192.168.10.1)
```

**Key Exchange:**
- Generated during Terraform planning
- Stored in Terraform state (encrypted at rest recommended)
- Passed to nodes via user_data scripts
- All keys are 256-bit X25519 keys

## 2. Kubernetes Architecture

### 2.1 Cluster Topology

```
K3s Cluster (Single cluster spanning AWS & GCP)
│
├─ Master Node(s)
│  └─ AWS K3s Control Plane (t2.micro)
│     ├── API Server (port 6443)
│     ├── etcd (embedded)
│     ├── Controller Manager
│     ├── Scheduler
│     └── Kubelet
│
├─ Worker Nodes
│  ├─ AWS Agent 1 (t2.micro)
│  ├─ AWS Agent 2 (t2.micro) [optional]
│  └─ GCP Agent 1 (e2-micro)
│
└─ Network
   ├── Pod Network (Calico)
   │   ├── Pod CIDR: 10.0.0.0/8
   │   ├── Service CIDR: 10.43.0.0/16
   │   └── Cross-cloud BGP peering
   │
   ├── WireGuard Tunnel (192.168.10.0/24)
   │   └── Encrypted node-to-node communication
   │
   └── Public Internet (UDP 123)
       └── NTP Service exposure
```

### 2.2 Service Deployment Architecture

```
Namespace: ntp-server
│
├─ Deployment: ntp-server
│  ├── Replicas: 2-10 (HPA controlled)
│  ├── Image: cturra/ntp:latest
│  ├── CPU: 100m (request) / 200m (limit)
│  ├── Memory: 64Mi (request) / 128Mi (limit)
│  ├── Health Checks:
│  │  ├── Liveness Probe: ntpq -p | grep SYNC
│  │  └── Readiness Probe: ntpq -p | grep "SYNC|GPS"
│  └── Anti-Affinity:
│     ├── Preferred: Different nodes
│     └── Preferred: Different cloud providers
│
├─ Service: ntp-server
│  ├── Type: LoadBalancer
│  ├── Protocol: UDP
│  ├── Port: 123
│  ├── External Traffic Policy: Local
│  └── Endpoints: All NTP pods
│
├─ HPA: ntp-server-hpa
│  ├── Min Replicas: 2
│  ├── Max Replicas: 10
│  ├── CPU Target: 80%
│  ├── Memory Target: 80%
│  └── Scale-up/down behavior:
│     ├── Scale up: Immediately (0s stabilization)
│     └── Scale down: 5min stabilization
│
├─ PodDisruptionBudget: ntp-server-pdb
│  ├── Min Available: 1
│  └── Applies to: ntp-server pods
│
└─ NetworkPolicy
   ├── Ingress: UDP 123 from 0.0.0.0/0
   ├── Egress: DNS (53), NTP (123), HTTPS (443)
   └── Isolation: Default deny + explicit allow
```

### 2.3 Monitoring Stack

```
Namespace: monitoring
│
├─ Prometheus
│  ├── Scrape Interval: 15s
│  ├── Retention: 30 days
│  ├── Targets:
│  │  ├── Kubernetes API servers
│  │  ├── Kubelet metrics
│  │  ├── Pod metrics
│  │  ├── NTP service (port 9100)
│  │  └── Custom exporters
│  └── Storage: emptyDir (ephemeral)
│
├─ Grafana
│  ├── Data Source: Prometheus
│  ├── Dashboards:
│  │  ├── K3s Cluster Status
│  │  ├── NTP Server Health
│  │  ├── Pod Resource Utilization
│  │  ├── Cross-cloud Network Stats
│  │  └── Cost Tracking
│  ├── Alerting: Via Prometheus rules
│  └── Access: Port 3000 (LoadBalancer)
│
└─ Metrics Server
   ├── Aggregates kubelet metrics
   ├── Enables HPA calculations
   └── Update interval: 15s
```

## 3. Infrastructure as Code (Terraform)

### 3.1 Terraform Module Structure

```
terraform/
├── root module
│  ├── main.tf (orchestration)
│  ├── variables.tf (input)
│  └── outputs.tf (values)
│
├── wireguard module
│  ├── main.tf (key generation)
│  ├── variables.tf (config)
│  └── outputs.tf (keys for other modules)
│
├── aws module
│  ├── main.tf
│  │  ├── VPC creation
│  │  ├── EC2 instances
│  │  ├── Security groups
│  │  ├── Elastic IPs
│  │  ├── CloudWatch logs
│  │  └── Budget alerts
│  ├── variables.tf
│  ├── vpc_module.tf (inline VPC)
│  └── user_data/
│     ├── k3s_control_plane.sh
│     └── k3s_agent.sh
│
└── gcp module
   ├── main.tf
   │  ├── VPC network
   │  ├── Firewall rules
   │  ├── Compute instances
   │  ├── Service account + IAM
   │  └── Budget alerts
   ├── variables.tf
   └── user_data/
      └── k3s_agent_gcp.sh
```

### 3.2 Terraform Execution Flow

```
1. terraform plan
   │
   ├── Generate WireGuard keys (wireguard module)
   ├── Plan AWS resources (EC2, VPC, Security Groups)
   ├── Plan GCP resources (Compute, VPC, Firewall)
   └── Output: tfplan file

2. terraform apply tfplan
   │
   ├── Apply WireGuard module
   │  └── Output keys for other modules
   │
   ├── Apply AWS module
   │  ├── Create VPC (10.0.0.0/16)
   │  ├── Create Security Groups
   │  ├── Launch EC2 instances
   │  ├── Attach EBS volumes
   │  ├── Allocate Elastic IPs
   │  └── Execute user_data scripts
   │
   ├── Apply GCP module
   │  ├── Create VPC (10.1.0.0/16)
   │  ├── Create Firewall rules
   │  ├── Launch Compute instance
   │  └── Execute startup script
   │
   └── Output: Cluster endpoint, IPs, kubeconfig
```

### 3.3 State Management

**Default (Local):**
```bash
terraform/terraform.tfstate (local file)
└── Contains all resource state
    ├── Sensitive data: WireGuard keys, API tokens
    ├── Risk: Loss if file corrupted
    └── Recommendation: Backup frequently
```

**Remote State (Recommended):**
```bash
# AWS S3 backend (uncomment in root main.tf)
backend "s3" {
  bucket         = "your-terraform-state"
  key            = "ntp-mc-nlb/terraform.tfstate"
  region         = "us-east-1"
  encrypt        = true
  dynamodb_table = "terraform-locks"  # For locking
}

# GCP GCS backend
backend "gcs" {
  bucket = "your-terraform-state"
  prefix = "ntp-mc-nlb"
}
```

## 4. Security Architecture

### 4.1 Network Security Layers

```
┌────────────────────────────────────────┐
│  Internet (Public)                      │
│  Source: 0.0.0.0/0                     │
└──────────────────┬─────────────────────┘
                   │
                   ▼
        ┌──────────────────────┐
        │  Cloud Firewalls     │
        │  (AWS Security Groups│
        │   GCP Firewall)      │
        ├──────────────────────┤
        │ Ingress Rules:       │
        │ • 123/UDP from 0/0   │
        │ • 6443/TCP (K3s API) │
        │ • 22/TCP (SSH)       │
        │ • 51820/UDP (WG)     │
        └──────────────────────┘
                   │
                   ▼
        ┌──────────────────────┐
        │  K8s NetworkPolicy   │
        │  (Calico)            │
        ├──────────────────────┤
        │ Pod-Level Rules:     │
        │ • Default deny       │
        │ • Explicit allow NTP │
        │ • DNS exceptions     │
        └──────────────────────┘
                   │
                   ▼
        ┌──────────────────────┐
        │  NTP Service Pods    │
        │  (Sandboxed)         │
        └──────────────────────┘
```

### 4.2 Encryption Layers

```
NTP Queries (Public)
│
├─ Transit: None (standard UDP/IP)
└─ Purpose: Public NTP service

Inter-Cloud Communication
│
├─ Transit: WireGuard VPN (AES-256)
├─ Keys: X25519 (256-bit)
├─ Authentication: Cryptographic
└─ Purpose: Secure node-to-node tunnel

Data at Rest
│
├─ EBS volumes: AES-256 (AWS)
├─ GCP Persistent disks: AES-256
├─ K3s etcd: Encrypted (via WireGuard tunnel)
└─ Terraform state: Encrypted (S3 recommended)
```

### 4.3 Access Control

```
SSH Access
├─ AWS: Security Group restricted
│  └─ allowed_ssh_cidrs (configurable)
│
├─ GCP: Firewall rules + OS Login
│  └─ Service account IAM
│
└─ Recommendation: Whitelist only your IP

K3s API Access
├─ Authentication: K3s token (env var)
├─ Authorization: RBAC (default)
└─ Network: Only reachable from WireGuard tunnel

AWS Resources
├─ IAM: EC2 instance roles (not root keys)
├─ Policies: Least privilege
│  └─ CloudWatch logs, Budgets
└─ CloudTrail: Audit logging

GCP Resources
├─ Service Accounts: Least privilege
├─ IAM Roles: Custom (monitoring, compute)
└─ Cloud Audit Logs: Activity tracking
```

## 5. Autoscaling & Performance

### 5.1 Horizontal Pod Autoscaling (HPA)

```
NTP Requests
│
▼
Load on Pods
│
├─ CPU Utilization: 80% threshold
├─ Memory Utilization: 80% threshold
│
▼
HPA Decision
├─ If above threshold:
│  ├─ Scale up 100% per 30 seconds
│  └─ Max replicas: 10
│
├─ If below threshold (5min stable):
│  ├─ Scale down 50% per minute
│  └─ Min replicas: 2
│
▼
New Pods Scheduled
├─ AntiAffinity: Spread across nodes/clouds
├─ Constraints: Free tier resource limits
└─ Health checks: 30s delay before traffic
```

### 5.2 Resource Limits (Free Tier Optimization)

```
NTP Pod (per replica)
├─ CPU Request: 100m (0.1 core)
├─ CPU Limit: 200m (0.2 core)
├─ Memory Request: 64Mi
├─ Memory Limit: 128Mi
│
└─ Reasoning:
   ├─ NTP is lightweight protocol
   ├─ Stratum 2 server minimal compute
   ├─ Most resource = network I/O
   └─ Limits prevent cluster exhaustion

t2.micro Node Capacity
├─ vCPU: 1 (burst capable)
├─ Memory: 1 GB total
├─ Available for NTP: ~800 MB RAM
│
└─ Maximum NTP Replicas: 10 pods
   ├─ 10 × 128Mi = 1.28 GB (exceeds node)
   ├─ In practice: 6-8 pods per t2.micro
   └─ Scaling across AWS + GCP nodes

e2-micro Node Capacity
├─ vCPU: 0.25-2 (shared)
├─ Memory: 1 GB total
└─ Maximum NTP Replicas: 6-8 pods
```

### 5.3 Performance Metrics

```
Free Tier NTP Performance (Expected)
├─ Throughput: 500-1000 requests/sec per pod
├─ Latency: <1ms (same cloud), <50ms (cross-cloud)
├─ Jitter: <10ms
├─ Uptime: 99.5% (single zone)
├─ Network: Limited by:
│  ├─ AWS egress: 100 GB/month free
│  ├─ GCP egress: 1 GB/month free
│  └─ Cross-cloud: $0.21/GB (cost limiting factor)
│
└─ Post-Free Tier
   ├─ AWS t2.micro: ~2000-3000 req/sec
   ├─ GCP e2-micro: ~1000-1500 req/sec
   └─ Combined: ~4000-5000 req/sec
```

## 6. Cost Architecture

### 6.1 Cost Breakdown Model

```
Monthly Costs (Post-Free Tier)
│
├─ AWS Compute
│  ├─ t2.micro × 2 instances: 730h × $0.0116/h = $16.94
│  ├─ EBS gp3 (20GB × 2): 40 × $0.10 = $4.00
│  └─ Subtotal: $20.94
│
├─ GCP Compute
│  ├─ e2-micro × 1 instance: 730h × $0.0238/h = $17.37
│  ├─ Persistent disk (20GB): 20 × $0.04 = $0.80
│  └─ Subtotal: $18.17
│
├─ Data Transfer
│  ├─ Inbound (all clouds): $0 (free)
│  ├─ Outbound AWS: 50GB × $0.09 = $4.50
│  ├─ Outbound GCP: 10GB × $0.12 = $1.20
│  ├─ Cross-cloud inter-region: ~$5/month
│  └─ Subtotal: $10.70
│
├─ Networking
│  ├─ AWS Public IP (unused): $0.005/h × 730 = $3.65
│  ├─ GCP Public IP (unused): $0.01/h × 730 = $7.30
│  ├─ VPC peering: $0 (not using direct peering)
│  └─ Subtotal: $10.95
│
├─ Monitoring & Logging
│  ├─ CloudWatch Logs (AWS): ~$1/month
│  ├─ Prometheus (self-hosted): $0
│  └─ Subtotal: $1.00
│
└─ TOTAL: ~$62/month
```

### 6.2 Free Tier Rotation Strategy

```
Free Tier Available Hours/Month
├─ AWS t2.micro: 750 hours (12-month free)
├─ GCP e2-micro: 730 hours (always free)
└─ Combined: 1480 hours (unlimited with rotation)

Rotation Schedule
├─ Monday-Wednesday (72 hours)
│  ├─ AWS instances: ON (16h/day × 3 = 48h)
│  ├─ GCP instance: OFF
│  └─ Running hours: 48h
│
├─ Thursday-Friday (48 hours)
│  ├─ AWS instances: Mixed (8h/day × 2 = 16h)
│  ├─ GCP instance: ON (24h/day × 2 = 48h)
│  └─ Running hours: 48h + 16h = 64h
│
├─ Saturday-Sunday (48 hours)
│  ├─ AWS instances: Mixed (8h/day × 2 = 16h)
│  ├─ GCP instance: ON (24h/day × 2 = 48h)
│  └─ Running hours: 48h + 16h = 64h
│
└─ Monthly Total
   ├─ AWS instances: 48 + 16 + 16 = 80 hours << 750 ✓
   ├─ GCP instance: 0 + 48 + 48 = 96 hours << 730 ✓
   └─ Both free tiers satisfied
```

## 7. Disaster Recovery

### 7.1 Backup Strategy

```
Backup Scope
├─ Terraform State
│  ├─ Frequency: Continuous (remote state)
│  ├─ Location: S3 / GCS encrypted
│  ├─ Retention: 30 days versions
│  └─ Recovery: terraform apply with state
│
├─ K3s Configuration
│  ├─ Location: Embedded in Git + remote backup
│  ├─ Frequency: Per deployment
│  └─ Recovery: kubectl apply -f manifests/
│
├─ NTP Service Data
│  ├─ Notes: Stateless (no DB)
│  ├─ Logs: CloudWatch / application logs
│  └─ Recovery: Restart pods (rebuilt automatically)
│
└─ Secrets & Keys
   ├─ WireGuard keys: Terraform state (encrypted)
   ├─ K3s token: Terraform state (encrypted)
   ├─ Location: Secrets manager (optional)
   └─ Recovery: Re-generate via Terraform
```

### 7.2 Disaster Scenarios

```
Scenario: Single Pod Failure
├─ Detection: Kubelet notices crash
├─ Recovery: ReplicaSet creates new pod (automatic)
└─ Time: <30 seconds

Scenario: Node Failure (AWS)
├─ Detection: Kubelet not responding (5min timeout)
├─ Recovery: Pods evicted, rescheduled to GCP node
├─ Degradation: Potential overload on GCP
└─ Time: 5-10 minutes
└─ Mitigation: HPA scales GCP node if needed

Scenario: Network Partition (AWS-GCP)
├─ Detection: WireGuard timeout / latency spike
├─ Recovery: Each cloud serves independently
├─ Degradation: No cross-cloud communication
└─ Time: Immediate / ~30 seconds detection

Scenario: Cloud Region Failure
├─ AWS Region Down: GCP continues NTP service
│  ├─ Loss: AWS compute (recovery time ~5min)
│  └─ Service: GCP maintains minimal service
│
├─ GCP Region Down: AWS continues NTP service
│  ├─ Loss: GCP compute
│  └─ Service: AWS maintains full service
│
└─ Both Down: Complete failure (rare)
   └─ Recovery: Terraform redeploy (10-15 min)

Scenario: Budget Exceeded - Auto Shutdown
├─ Trigger: Cost > 100% of budget
├─ Action: Alert email + logs
├─ Manual: Run emergency_shutdown.sh
└─ Effect: All infrastructure destroyed
```

### 7.3 Recovery Procedures

```
Procedure: Redeploy Cluster from Scratch
└─ Time: 10-15 minutes
   
   1. terraform destroy (2 min)
   2. terraform apply (5 min for EC2 launch)
   3. K3s initialization (5 min)
   4. Application deployment (2 min)
   5. DNS/pool.ntp.org update (manual)

Procedure: Scale Up During Spike
└─ Time: 30-60 seconds (HPA automatic)
   
   1. Pods exceed 80% CPU/memory threshold
   2. HPA creates new pods
   3. New pods scheduled to available nodes
   4. Pods enter ready state
   5. Service load balancer includes new pods

Procedure: Failover to Single Cloud
└─ Time: Immediate (no change needed)
   
   1. If AWS down: GCP continues
   2. If GCP down: AWS continues
   3. Monitor metrics for degradation
   4. Manually scale remaining cloud if needed
```

## 8. Performance Tuning

### 8.1 NTP Server Optimization

```
Kernel Tuning (Applied via user_data)
├─ UDP Buffer: net.core.rmem_max = 134217728
├─ Socket backlog: net.ipv4.tcp_max_syn_backlog = 4096
├─ Connection tracking: nf_conntrack limits
└─ NIC interrupt handling: RPS / RFS

NTP Daemon Optimization (ntp.conf)
├─ Minpoll: 4 (16 sec minimum query)
├─ Maxpoll: 6 (64 sec maximum query)
├─ Burst mode: Enabled for faster sync
├─ Interleave mode: Disabled (extra latency)
└─ Access lists: Restrictive (only NTP queries)

Container Optimization
├─ CPU shares: Default (fair share)
├─ Memory: Hard limits prevent OOM
├─ Network: MTU 1500 (standard)
└─ Logging: Minimal (avoid latency)
```

### 8.2 Kubernetes Tuning

```
API Server Optimization
├─ --max-requests-inflight: 1000 (default)
├─ --max-mutating-requests-inflight: 200
├─ --audit-log-maxage: 7 days
└─ --event-ttl: 1h

Kubelet Tuning
├─ --max-pods: 110 (default, fine for NTP)
├─ --pod-gc-delay: 60s
├─ --cadvisor-port: 0 (disabled, unused)
└─ --streaming-connection-idle-timeout: 5m

Controller Manager Tuning
├─ --concurrent-service-syncs: 1
├─ --concurrent-deployment-syncs: 5
└─ --horizontal-pod-autoscaler-sync-period: 15s
```

## 9. Compliance & Standards

### 9.1 NTP Compliance

```
RFC 5905 Compliance
├─ Protocol: NTPv4 (supported by cturra/ntp image)
├─ Packet Format: 48-byte fixed format
├─ Authentication: Optional (not used by default)
├─ Error Handling: REFUSAL status implemented
└─ Stratum: Configured as Stratum 2

pool.ntp.org Requirements
├─ Static IPv4: ✓ (Elastic IP allocated)
├─ IPv6 Support: ✓ (AWS dual-stack VPC)
├─ 24/7 Uptime: ✓ (99.5% expected)
├─ Accuracy: ✓ (<10ms typical)
├─ Bandwidth: ✓ (Minimal protocol)
├─ Monitoring: ✓ (Prometheus metrics)
└─ Support: ✓ (Respond to pool inquiries)
```

### 9.2 Security Standards

```
CIS Kubernetes Benchmarks
├─ Pod security policies: ✓ (NetworkPolicy)
├─ RBAC: ✓ (K3s default + audit)
├─ Network segmentation: ✓ (Calico policies)
├─ Encryption in transit: ✓ (WireGuard)
└─ Encryption at rest: ✓ (EBS / Persistent disk)

Cloud Security Standards
├─ AWS Well-Architected: ✓ (Cost optimized)
├─ GCP Security Best Practices: ✓ (IAM, Firewall)
└─ Data residency: ✓ (US regions only)
```

---

**Version:** 1.0
**Last Updated:** January 2026
**Document Owner:** Infrastructure Team
